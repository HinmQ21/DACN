# Multi-Agent Medical QA System with Medprompt

Há»‡ thá»‘ng multi-agent sá»­ dá»¥ng Gemini, LangChain vÃ  LangGraph Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i y táº¿ vÃ  Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c benchmark nhÆ° MedQA, PubMedQA.

**TÃ­ch há»£p Medprompt** - phÆ°Æ¡ng phÃ¡p prompt engineering tiÃªn tiáº¿n tá»« Microsoft Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn cÃ¡c bÃ i toÃ¡n y táº¿.

## ğŸŒŸ TÃ­nh nÄƒng má»›i: Medprompt Integration

### 1. Dynamic Few-shot Selection
- Tá»± Ä‘á»™ng tÃ¬m cÃ¡c cÃ¢u há»i tÆ°Æ¡ng tá»± tá»« training set
- Sá»­ dá»¥ng embedding model y táº¿ (PubMedBERT)
- K-NN retrieval Ä‘á»ƒ chá»n examples phÃ¹ há»£p nháº¥t

### 2. Self-Generated Chain-of-Thought (CoT)
- Táº¡o chuá»—i suy luáº­n chi tiáº¿t
- Há»c tá»« examples tÆ°Æ¡ng tá»±
- PhÃ¢n tÃ­ch tá»«ng option má»™t cÃ¡ch logic

### 3. Choice Shuffling Ensemble
- Giáº£m bias vá»‹ trÃ­ trong cÃ¢u há»i tráº¯c nghiá»‡m
- Cháº¡y nhiá»u variants vá»›i options Ä‘Æ°á»£c shuffle
- Majority voting Ä‘á»ƒ chá»n Ä‘Ã¡p Ã¡n cuá»‘i cÃ¹ng

## Workflow

```
            Input Question 
                 â†“
      [Coordinator Agent]
         â”œâ”€â”€ PhÃ¢n tÃ­ch cÃ¢u há»i
         â””â”€â”€ ğŸ“Œ Dynamic Few-shot Selection (K-NN)
                 â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â†“                               â†“
Web Search              [Reasoning Agent]
                            â””â”€â”€ ğŸ“Œ Self-Generated CoT
 â†“                               â†“
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
          [Validator Agent]
              â””â”€â”€ ğŸ“Œ Choice Shuffling Ensemble
                 â†“
         Answer Generator
                 â†“
              Output
```

## Cáº¥u trÃºc thÆ° má»¥c

```
DACN/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ coordinator.py       # + Dynamic Few-shot Selection
â”‚   â”œâ”€â”€ web_search.py        # Tavily + PubMed search
â”‚   â”œâ”€â”€ reasoning.py         # + Self-Generated CoT
â”‚   â”œâ”€â”€ validator.py         # + Choice Shuffling Ensemble
â”‚   â””â”€â”€ answer_generator.py  
â”œâ”€â”€ workflows/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ medical_qa_graph.py  # LangGraph workflow vá»›i Medprompt
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ medqa_eval.py        
â”‚   â””â”€â”€ pubmedqa_eval.py     
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py            # Cáº¥u hÃ¬nh + Medprompt settings
â”‚   â”œâ”€â”€ metrics.py           
â”‚   â”œâ”€â”€ embedding_service.py # ğŸ†• Vector embeddings
â”‚   â”œâ”€â”€ knn_retriever.py     # ğŸ†• K-NN retrieval
â”‚   â””â”€â”€ ensemble.py          # ğŸ†• Voting mechanisms
â”œâ”€â”€ data/
â”‚   â””â”€â”€ knowledge_base/      # ğŸ†• Embedded training examples
â”œâ”€â”€ build_knowledge_base.py  # ğŸ†• Script build index
â”œâ”€â”€ run_benchmark.py         # + Medprompt options
â”œâ”€â”€ example_usage.py         
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## CÃ i Ä‘áº·t

### BÆ°á»›c 1: CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### BÆ°á»›c 2: Cáº¥u hÃ¬nh API Keys

1. Táº¡o file `.env`:
```env
GOOGLE_API_KEY=your_gemini_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp
TEMPERATURE=0.3

# Medprompt settings
ENABLE_FEW_SHOT=true
ENABLE_COT=true
ENABLE_ENSEMBLE=true
```

2. Láº¥y API Keys:
   - **Google Gemini API**: https://makersuite.google.com/app/apikey
   - **Tavily API**: https://tavily.com/

### BÆ°á»›c 3: Build Knowledge Base (cho Few-shot Selection)
```bash
python build_knowledge_base.py --train_file MedQA/4_options/phrases_no_exclude_train.jsonl
```

### BÆ°á»›c 4: Kiá»ƒm tra cÃ i Ä‘áº·t
```bash
python example_usage.py
```

## Sá»­ dá»¥ng

### Cháº¡y má»™t cÃ¢u há»i Ä‘Æ¡n láº»:
```bash
python main.py --question "What is the most common cause of pneumonia?"
```

### Cháº¡y benchmark vá»›i Medprompt:
```bash
python run_benchmark.py --dataset medqa --max-samples 100
```

### Cháº¡y benchmark KHÃ”NG cÃ³ Medprompt (Ä‘á»ƒ so sÃ¡nh):
```bash
python run_benchmark.py --dataset medqa --max-samples 100 --no-medprompt
```

### TÃ¹y chá»n Medprompt:
```bash
# Disable tá»«ng feature
python run_benchmark.py --no-few-shot    # KhÃ´ng dÃ¹ng few-shot
python run_benchmark.py --no-cot         # KhÃ´ng dÃ¹ng CoT
python run_benchmark.py --no-ensemble    # KhÃ´ng dÃ¹ng ensemble

# TÃ¹y chá»‰nh parameters
python run_benchmark.py --few-shot-k 5 --ensemble-variants 7
```

## Cáº¥u hÃ¬nh Medprompt

Xem chi tiáº¿t táº¡i:
- [CONFIG_GUIDE.md](CONFIG_GUIDE.md) - Cáº¥u hÃ¬nh tá»•ng há»£p
- [MEDPROMPT_GUIDE.md](MEDPROMPT_GUIDE.md) - HÆ°á»›ng dáº«n Medprompt chi tiáº¿t

### Quick Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `ENABLE_FEW_SHOT` | true | Báº­t few-shot selection |
| `FEW_SHOT_K` | 3 | Sá»‘ examples tÆ°Æ¡ng tá»± |
| `ENABLE_COT` | true | Báº­t Chain-of-Thought |
| `ENABLE_ENSEMBLE` | true | Báº­t choice shuffling |
| `ENSEMBLE_VARIANTS` | 5 | Sá»‘ variants |

## CÃ¡c Agent

1. **Coordinator**: PhÃ¢n tÃ­ch cÃ¢u há»i + **Dynamic Few-shot Selection**
2. **Web Search Agent**: TÃ¬m kiáº¿m tá»« Tavily vÃ  PubMed
3. **Reasoning Agent**: Suy luáº­n logic + **Self-Generated CoT**
4. **Validator**: Kiá»ƒm tra tÃ­nh nháº¥t quÃ¡n + **Choice Shuffling Ensemble**
5. **Answer Generator**: Tá»•ng há»£p cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng

## Metrics

- **Accuracy**: Tá»· lá»‡ cÃ¢u tráº£ lá»i Ä‘Ãºng
- **F1 Score**: Harmonic mean cá»§a Precision vÃ  Recall
- **Precision/Recall**: Äá»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ phá»§
- **Response Time**: Thá»i gian xá»­ lÃ½ trung bÃ¬nh
- **Confidence Score**: Äá»™ tin cáº­y cá»§a cÃ¢u tráº£ lá»i
- **Ensemble Consistency**: Äá»™ nháº¥t quÃ¡n giá»¯a cÃ¡c predictions (má»›i)

## VÃ­ Dá»¥ Output

```
Question: What is the most common cause of pneumonia in adults?

Answer: B

Explanation: Streptococcus pneumoniae is the most common bacterial cause 
of community-acquired pneumonia in adults.

Confidence: 0.89
Sources: 8

--- Medprompt Info ---
Few-shot examples used: 3
CoT reasoning: True
Ensemble used: True
Ensemble consistency: 0.80
Predictions: ['B', 'B', 'B', 'B', 'A']
Vote distribution: {'B': 0.8, 'A': 0.2}
```

## TÃ i liá»‡u

- [CONFIG_GUIDE.md](CONFIG_GUIDE.md) - HÆ°á»›ng dáº«n cáº¥u hÃ¬nh
- [MEDPROMPT_GUIDE.md](MEDPROMPT_GUIDE.md) - HÆ°á»›ng dáº«n Medprompt
- [DEVELOPMENT_GUIDE.md](DEVELOPMENT_GUIDE.md) - HÆ°á»›ng dáº«n phÃ¡t triá»ƒn
- [architecture_diagram.md](architecture_diagram.md) - Kiáº¿n trÃºc chi tiáº¿t

## Roadmap

- [x] ~~Triá»ƒn khai Medprompt (Few-shot, CoT, Ensemble)~~
- [ ] ThÃªm support cho hÃ¬nh áº£nh y táº¿ (X-ray, CT, MRI)
- [ ] TÃ­ch há»£p thÃªm datasets (MedMCQA, MMLU-Medical)
- [ ] Web UI vá»›i Streamlit/Gradio
- [ ] API server vá»›i FastAPI

## References

- [Medprompt Paper](https://arxiv.org/abs/2311.16452) - Microsoft Research
- [MedQA Dataset](https://github.com/jind11/MedQA)
- [Sentence Transformers](https://www.sbert.net/)
